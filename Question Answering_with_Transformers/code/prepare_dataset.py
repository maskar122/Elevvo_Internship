import argparse
import json
import os
from typing import List, Dict

from datasets import Dataset, DatasetDict
from transformers import DistilBertTokenizerFast


def parse_squad(path: str) -> List[Dict]:
    """
    ÙŠØ­ÙˆÙ‘Ù„ Ù…Ù„Ù SQuAD v1.1 (Ø¨Ù†ÙØ³ Ù‡ÙŠÙƒÙ„Ø© Ø³ØªØ§Ù†ÙÙˆØ±Ø¯) Ø¥Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø£Ù…Ø«Ù„Ø© Ù…Ø³Ø·Ø­Ø©.
    ÙƒÙ„ Ù…Ø«Ø§Ù„: {id, context, question, answer_text, answer_start}
    """
    with open(path, "r", encoding="utf-8") as f:
        raw = json.load(f)

    examples = []
    for article in raw["data"]:
        for para in article["paragraphs"]:
            context = para["context"]
            for qa in para["qas"]:
                qid = qa.get("id", "")
                # SQuAD v1.1 Ø¹Ø§Ø¯Ø©Ù‹ ÙÙŠÙ‡Ø§ Ø¥Ø¬Ø§Ø¨Ø© ÙˆØ§Ø­Ø¯Ø© (Ù„ÙƒÙ† Ø£Ø­ÙŠØ§Ù†Ù‹Ø§ Ø£ÙƒØ«Ø± Ù…Ù† ÙˆØ§Ø­Ø¯Ø©)
                # Ù‡Ù†Ø§ Ù†Ø£Ø®Ø° Ø£ÙˆÙ„ Ø¥Ø¬Ø§Ø¨Ø© ÙÙ‚Ø· (ÙƒÙ…Ø§ Ù‡Ùˆ Ø´Ø§Ø¦Ø¹ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¨Ø³ÙŠØ·)
                ans = qa["answers"][0]
                examples.append(
                    {
                        "id": qid,
                        "context": context,
                        "question": qa["question"],
                        "answer_text": ans["text"],
                        "answer_start": ans["answer_start"],
                    }
                )
    return examples


def build_hf_dataset(train_path: str, dev_path: str) -> DatasetDict:
    train_examples = parse_squad(train_path)
    dev_examples = parse_squad(dev_path)

    train_ds = Dataset.from_list(train_examples)
    dev_ds = Dataset.from_list(dev_examples)

    return DatasetDict({"train": train_ds, "validation": dev_ds})


def prepare_features(tokenizer: DistilBertTokenizerFast, max_length=384, doc_stride=128):
    """
    ØªÙØ±Ø¬Ø¹ Ø¯Ø§Ù„Ø© map Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯Ø§ØªØ§ Ø¥Ù„Ù‰ Ù…ÙŠØ²Ø§Øª ØªØ¯Ø±ÙŠØ¨ Ø¬Ø§Ù‡Ø²Ø©:
    - ØªØ±Ù…ÙŠØ² (question, context)
    - Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„ÙÙ‚Ø±Ø§Øª Ø§Ù„Ø·ÙˆÙŠÙ„Ø© Ø¹Ø¨Ø± overflow + doc_stride
    - Ø­Ø³Ø§Ø¨ start_positions / end_positions Ø¨Ø§Ù„Ø§Ø¹ØªÙ…Ø§Ø¯ Ø¹Ù„Ù‰ offset_mapping
    """
    def _fn(examples):
        questions = [q.strip() for q in examples["question"]]
        contexts = examples["context"]
        answers = examples["answer_text"]
        answer_starts = examples["answer_start"]

        tokenized = tokenizer(
            questions,
            contexts,
            truncation="only_second",
            max_length=max_length,
            stride=doc_stride,
            return_overflowing_tokens=True,
            return_offsets_mapping=True,
            padding="max_length",
        )

        # Ù†Ø­ØªØ§Ø¬ Ø±Ø¨Ø· ÙƒÙ„ feature Ø¨Ø§Ù„Ù…Ø«Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ (Ù„Ø£Ù† overflow ÙŠÙ†Ø³Ø® Ø§Ù„Ù…Ø«Ø§Ù„ Ù„Ø¹Ø¯Ø© Ø´Ø±Ø§Ø¦Ø­)
        sample_mapping = tokenized.pop("overflow_to_sample_mapping")
        offset_mapping = tokenized.pop("offset_mapping")

        start_positions = []
        end_positions = []

        for i, offsets in enumerate(offset_mapping):
            # Ù‡Ø°Ø§ Ø§Ù„Ù€ feature ÙŠØ¹ÙˆØ¯ Ù„Ø£ÙŠ Ø¹ÙŠÙ†Ø© Ø£ØµÙ„Ù‹Ø§ØŸ
            sample_idx = sample_mapping[i]
            answer_start_char = answer_starts[sample_idx]
            answer_text = answers[sample_idx]
            answer_end_char = answer_start_char + len(answer_text)

            # Ù†Ø­Ø¯Ø¯ Ø£ÙŠ ØªÙˆÙƒÙ†Ø§Øª ØªØ®Øµ Ø§Ù„Ù€ context (sequence_ids: 0=>question, 1=>context, None=>special)
            sequence_ids = tokenized.sequence_ids(i)

            # Ø­Ø¯Ù‘Ø¯ Ø­Ø¯ÙˆØ¯ Ø§Ù„Ù€ context Ø¯Ø§Ø®Ù„ Ø§Ù„Ø³Ù„Ø³Ù„Ø© Ø§Ù„Ù…Ø±Ù…Ù‘Ø²Ø©
            # Ø£ÙˆÙ„ token Ù„Ù„Ù€ context:
            context_start = 0
            while context_start < len(sequence_ids) and sequence_ids[context_start] != 1:
                context_start += 1
            # Ø¢Ø®Ø± token Ù„Ù„Ù€ context:
            context_end = len(sequence_ids) - 1
            while context_end >= 0 and sequence_ids[context_end] != 1:
                context_end -= 1

            # Ù„Ùˆ Ù„Ù… Ù†Ø¬Ø¯ Ø³ÙŠØ§Ù‚ ÙØ¹Ù„ÙŠ (Ù†Ø§Ø¯Ø±Ù‹Ø§)ØŒ Ø¹Ø§Ù„Ø¬ ÙƒÙ€ CLS (0,0)
            if context_start > context_end:
                start_positions.append(0)
                end_positions.append(0)
                continue

            # Ù„Ùˆ ÙƒØ§Ù†Øª Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø®Ø§Ø±Ø¬ Ù‡Ø°Ù‡ Ø§Ù„Ø´Ø±ÙŠØ­Ø© (Ø¨Ø³Ø¨Ø¨ Ø§Ù„Ù‚ØµÙ‘)ØŒ Ø¶Ø¹Ù‡Ø§ Ø¹Ù„Ù‰ CLS
            if not (offsets[context_start][0] <= answer_start_char and
                    offsets[context_end][1] >= answer_end_char):
                start_positions.append(0)
                end_positions.append(0)
                continue

            # ØªØ­Ø±ÙŠÙƒ start_positions Ø¥Ù„Ù‰ Ø£ÙˆÙ„ token ÙŠØºØ·ÙŠ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
            start_token = context_start
            while start_token <= context_end and offsets[start_token][0] <= answer_start_char:
                if offsets[start_token][1] > answer_start_char:
                    break
                start_token += 1

            # ØªØ­Ø±ÙŠÙƒ end_positions Ø¥Ù„Ù‰ Ø¢Ø®Ø± token ÙŠØºØ·ÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
            end_token = context_end
            while end_token >= context_start and offsets[end_token][1] >= answer_end_char:
                if offsets[end_token][0] < answer_end_char:
                    break
                end_token -= 1

            # Ø£Ø­ÙŠØ§Ù†Ù‹Ø§ Ø§Ù„Ù„ÙˆØ¬ÙŠÙƒ Ø£Ø¹Ù„Ø§Ù‡ ÙŠØ­ØªØ§Ø¬ Ø¶Ø¨Ø· Ù„Ù…ÙˆØ§Ø¶Ø¹ Ø§Ù„ØªÙ…Ø§Ø³ â€” Ø¨Ø¯ÙŠÙ„ Ø¢Ù…Ù†:
            # Ø§Ø¨Ø­Ø« Ø¹Ù† Ø£ÙˆÙ„/Ø¢Ø®Ø± ØªÙˆÙƒÙ† ÙŠØºØ·ÙŠ Ø£ÙŠ Ø¬Ø²Ø¡ Ù…Ù† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
            # (Ù„Ùˆ ØªØ­Ø¨ ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„ÙƒØªÙ„Ø© Ø£Ø¹Ù„Ø§Ù‡ Ø¨Ù‡Ø°Ø§ Ø§Ù„Ù†Ù‡Ø¬)
            if start_token > context_end or end_token < context_start:
                # fallback
                # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£ÙˆÙ„ ØªÙˆÙƒÙ† ÙŠØºØ·ÙŠ Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
                start_token = context_start
                while start_token <= context_end and offsets[start_token][0] <= answer_start_char:
                    start_token += 1
                start_token = max(context_start, start_token - 1)

                # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¢Ø®Ø± ØªÙˆÙƒÙ† ÙŠØºØ·ÙŠ Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
                end_token = context_end
                while end_token >= context_start and offsets[end_token][1] >= answer_end_char:
                    end_token -= 1
                end_token = min(context_end, end_token + 1)

            start_positions.append(start_token)
            end_positions.append(end_token)

        tokenized["start_positions"] = start_positions
        tokenized["end_positions"] = end_positions
        return tokenized

    return _fn


def main():
    parser = argparse.ArgumentParser(description="Prepare SQuAD v1.1 for QA fine-tuning")
    parser.add_argument("--train_path", type=str, default="train-v1.1.json", help="Path to train-v1.1.json")
    parser.add_argument("--dev_path", type=str, default="dev-v1.1.json", help="Path to dev-v1.1.json")
    parser.add_argument("--model_name", type=str, default="distilbert-base-uncased", help="Tokenizer model name")
    parser.add_argument("--max_length", type=int, default=384)
    parser.add_argument("--doc_stride", type=int, default=128)
    parser.add_argument("--out_dir", type=str, default="data/processed", help="Output directory for saved datasets")
    args = parser.parse_args()

    os.makedirs(args.out_dir, exist_ok=True)

    print("ğŸ”¹ Loading raw SQuAD json ...")
    ds = build_hf_dataset(args.train_path, args.dev_path)

    print("ğŸ”¹ Loading tokenizer:", args.model_name)
    tokenizer = DistilBertTokenizerFast.from_pretrained(args.model_name, use_fast=True)

    print("ğŸ”¹ Tokenizing with sliding window (doc_stride) ...")
    features_fn = prepare_features(
        tokenizer,
        max_length=args.max_length,
        doc_stride=args.doc_stride
    )

    tokenized = ds.map(
        features_fn,
        batched=True,
        remove_columns=ds["train"].column_names,  # Ù†Ø­Ø°Ù Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø®Ø§Ù… ÙˆÙ†ÙØ¨Ù‚ÙŠ Ø§Ù„Ù…ÙŠØ²Ø§Øª ÙÙ‚Ø·
        desc="Tokenizing"
    )

    # Ø­ÙØ¸ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
    train_out = os.path.join(args.out_dir, "train")
    val_out = os.path.join(args.out_dir, "validation")

    print(f"ğŸ’¾ Saving tokenized train to: {train_out}")
    tokenized["train"].save_to_disk(train_out)

    print(f"ğŸ’¾ Saving tokenized validation to: {val_out}")
    tokenized["validation"].save_to_disk(val_out)

    print("âœ… Done! Datasets are ready for training.")


if __name__ == "__main__":
    main()
